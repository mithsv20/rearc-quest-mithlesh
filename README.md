
---

# ğŸ“˜ Rearc Quest â€“ Data Engineering Assignment

This repository contains my solution for the Rearc Data Engineering Quest.
The solution implements an end-to-end data pipeline that:

* Ingests public COVID-19 time-series data
* Ingests country metadata from a REST API
* Stores curated datasets in Amazon S3
* Computes analytics using Apache Spark
* Produces a business-meaningful infection-rate report

The solution emphasizes scalability, idempotency, clean architecture, and production-style practices.

---

## ğŸ— Architecture Overview

**Part 1**
Ingest COVID time-series CSV files from public GitHub â†’ S3

**Part 2**
Ingest country metadata (population, region) from REST Countries API â†’ curated Parquet in S3 (partitioned by region)

**Part 3**
Spark analytics (Dockerized):

* Aggregate latest confirmed cases by country
* Join with population dataset
* Compute infection rate (%)
* Output Top 20 countries by infection rate

---

## ğŸ“‚ Project Structure

```
rearc-quest-mithlesh/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ s3_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ part1/
â”‚   â”‚   â””â”€â”€ ingest_covid.py
â”‚   â”‚
â”‚   â”œâ”€â”€ part2/
â”‚   â”‚   â””â”€â”€ ingest_countries.py
â”‚   â”‚
â”‚   â””â”€â”€ part3/
â”‚       â””â”€â”€ analytics.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Prerequisites

* Python 3.9+
* AWS Account (Free Tier)
* AWS CLI configured
* Docker Desktop

Verify:

```bash
aws --version
docker --version
python --version
```

---

## ğŸ” AWS Configuration

Configure credentials:

```bash
aws configure
```

Confirm:

```bash
aws s3 ls
```

---

## ğŸª£ S3 Bucket

Create a bucket (example):

```bash
aws s3 mb s3://quest-mithlesh
```

Update bucket name in:

```
src/common/config.py
```

---

## ğŸ Local Python Setup

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

# â–¶ Part 1 â€“ COVID Data Ingestion

Uploads latest COVID time-series files to S3 and keeps them in sync.

```bash
python -m src.part1.ingest_covid
```

Verify:

```bash
aws s3 ls s3://quest-mithlesh/part1/covid_timeseries/
```

---

# â–¶ Part 2 â€“ Countries Metadata Ingestion (Parquet)

Runs inside Spark container.

Build image:

```bash
docker build -t rearc-spark .
```

Run ingestion:

```bash
docker run -it \
-v $(pwd):/app \
-v ~/.aws:/root/.aws \
-e PYTHONPATH=/app \
-e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
-e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
rearc-spark \
/opt/spark/bin/spark-submit \
--conf spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID \
--conf spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY \
/app/src/part2/ingest_countries.py
```

Verify:

```bash
aws s3 ls s3://quest-mithlesh/part2/countries_parquet/
```

You should see partitioned folders:

```
region=Asia/
region=Europe/
region=Africa/
...
```

---

# â–¶ Part 3 â€“ Spark Analytics (Docker)

Produces Top 20 countries by infection rate.

```bash
docker run -it \
-v $(pwd):/app \
-v ~/.aws:/root/.aws \
-e PYTHONPATH=/app \
-e AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \
-e AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \
rearc-spark \
/opt/spark/bin/spark-submit \
--conf spark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID \
--conf spark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY \
/app/src/part3/analytics_lambda.py
```

Verify output:

```bash
aws s3 ls s3://quest-mithlesh/part3/outputs/infection_rate_report/
```

---

## ğŸ“Š Output Schema

```
country
region
population
total_cases
infection_rate_pct
```

---

## ğŸ’¡ Design Choices

* Parquet for analytical storage
* Partitioning by region
* Broadcast join for small dimension table
* Column pruning & early filtering
* Dockerized Spark for reproducible execution
* No hard-coded filenames
* Idempotent ingestion

---

## ğŸ” Re-running Pipeline

The pipeline is safe to re-run:

* Part 1 syncs only new files
* Part 2 overwrites curated Parquet
* Part 3 overwrites final report

---

## ğŸš€ Future Enhancements

* Orchestration with Airflow / Step Functions
* CI pipeline for linting & tests
* Data quality checks
* IAM role-based auth

---

If you want, next we can move to **Part 4 (Automation / IaC)** and add a lightweight orchestration layer.
